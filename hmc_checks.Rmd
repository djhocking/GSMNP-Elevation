---
title: "Appendix B: HMC Diagnostics"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Script for checking MCMC/HMC diagnostics - this might be better as an Rmd to just have one output for supplemental publication

```{r}
#----- Load Libraries ------

library(rstan)
library(dplyr)
library(ggplot2)
library(tidyr)
# library(tidybayes)
library(loo)
# library(rstanarm)
library(bayesplot)
library(parallel)
library(kableExtra)
```

## *Plethodon jordani*

```{r}
#----- Load Data and Results -----

load(file = "Data/Derived/settings.RData")
load("Data/Derived/stan_prep.RData")
site_od_full_pjor <- readRDS(file = "Results/Stan/final_od_pjor_hmc.Rds")
```

## View Traceplots

 <!---
# (consider alt. see Simpson/Aki comments) because long enough chains almost always look "good" rank plots might be better going forward
-->

```{r}
traceplot(site_od_full_pjor, par = c("alpha0", "alpha1", "alpha2", "alpha3", "alpha4", "alpha5", "alpha6", "beta0", "beta1", "beta2", "beta3", "beta4", "beta5", "beta6", "sd_eps", "sd_p"))
```

## Check Domain Specific Expectations

### Check N for Truncation 

The augmentation to marginalize N out as a latent discrete requires setting an upper bound, K, to loop through. If K is too small the posterior will be truncated. Need to check for every N.

```{r}
# Too many N for one plot layout so need to separate into chunks of 16 - 24
N_min <- apply(PJOR5, 1, max) # max caught on any day, so know true N is at least that large

N <- rep(NA_character_, 159)
for(i in 1:159) {
  N[i] <- paste0("N[", i, "]")
}

n <- 25

for(i in 1:(ceiling(159 / n))) {
  k <- min(i*n, 159)
  j <- k - n
  hists <- mcmc_hist(site_od_full_pjor, pars= N[j:k], binwidth = 1)
  ggsave(hists, file = paste0("Results/Stan/pjor_N_hist", i, ".pdf"))
  print(hists)
}
dev.off()
```

### Check detection

```{r}
mcmc_dens(site_od_full_pjor, pars = c("mean_detection"))
```

## Check Divergences and Pairwise Correlations

```{r}
check_divergences(site_od_full_pjor)

color_scheme_set("darkgray")
# mcmc_scatter(
#   as.matrix(site_od_full_pjor),
#   pars = c("fit", "fit_new"), 
#   np = nuts_params(site_od_full_pjor), 
#   np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
# )

# mcmc_scatter(
#   as.matrix(site_od_full_pjor),
#   pars = c("eval[1,1]", "y_new[1,1]"), 
#   np = nuts_params(site_od_full_pjor), 
#   np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
# )

# look at variance terms because usually the hardest to fit
color_scheme_set("darkgray")
# mcmc_scatter(
#   site_od_full_pjor,
#   pars = c("sd_eps", "sd_p"), 
#   np = nuts_params(site_od_full_pjor), 
#   np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
# )

mcmc_pairs(
  site_od_full_pjor,
  pars = c("alpha0", "alpha1", "alpha2", "beta0", "sd_eps", "sd_p", "lp__"), 
  off_diag_args = list(alpha = 0.1),
  np = nuts_params(site_od_full_pjor), 
  np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
)

# pairs(site_od_full_pjor, pars = c("alpha0", "alpha1", "alpha2", "beta0", "sd_eps", "sd_p", "lp__"))
```

## Check Energy and Treedepth

```{r}
check_energy(site_od_full_pjor)

check_treedepth(site_od_full_pjor)
```

## Summarize Samples Sizes and Mixing

Effective samples sizes using rstan::monitor following Hoffman and Gelman (2014) to be more reliable and accurate (as in Monnahan et al. 2017) - UPDATE - now follow Vehtari et al. 2019.

effective sample sizes (should be > 100) - Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian BÃ¼rkner (2019). Rank-normalization, folding, and localization: An improved R-hat for assessing convergence of MCMC. arXiv preprint arXiv:1903.08008.

```{r, results='hide'}
mon <- rstan::monitor(site_od_full_pjor, warmup = nb)
# rstan:::print.simsummary(mon)
```

```{r}
max(mon$Rhat)
min(mon$Bulk_ESS)
min(mon$Tail_ESS)
```

## Posterior Predictive Checks

<!--
# Bayesian p-value check
plot(site_od_full_pjor, par = c("fit", "fit_new"))
pairs(site_od_full_pjor, pars = c("fit", "fit_new"))

fit <- rstan::extract(site_od_full_pjor, par = "fit")[[1]]
fit_new <- rstan::extract(site_od_full_pjor, par = "fit_new")[[1]]
# plot(fit, fit_new)

mean(fit_new > fit) # Bayesian p-value - not sure why it's much worse than JAGS output - especially in light of posterior predictive checks being great below
-->

Examine posterior prredictions of total counts across all 5 visits

```{r}
y_sum <- data.frame(transect = rownames(PJOR5), y_sum = rowSums(PJOR5), stringsAsFactors = FALSE)
y_sum_new <- rstan::extract(site_od_full_pjor, pars = c("y_new_sum"))

ppc_scatter_avg(y = y_sum$y_sum, yrep = y_sum_new[[1]]) + xlab("Mean expected count") + ylab("Observed count") + ggtitle("P. jordani") + theme(plot.title = element_text(face = "italic")) # average posterior across samples
ggsave("Results/Stan/ppc_pjor.pdf", dpi = 1000)

# ppc_scatter(y = y_sum$y_sum, yrep = y_sum_new[[1]][sample(1:nrow(y_sum_new[[1]]), size = 16, replace = FALSE) , ]) # iteration specific for 16 random otherwise too hard to plot. Maybe could overlay but probably unnecessary to see all iterations when things generally look very good
```

RMSE of posterior predictive

```{r}
sqrt(sum(((y_sum$y_sum - colMeans(y_sum_new[[1]]))^2) / nrow(y_sum)))
```

Posterior predictive check for each visit

```{r}
y_long <- PJOR5 %>%
  mutate(site = rownames(.)) %>%
  pivot_longer(starts_with("PJOR"),
               names_to = "visit",
               names_prefix = "PJOR")

y_new<- rstan::extract(site_od_full_pjor, pars = c("y_new"))[[1]]

y_new_mat <- matrix(NA_integer_, dim(y_new)[1], nrow(y_long))
for(i in 1:dim(y_new)[1]) {
  y_new_long <- y_new[i, , ] %>%
    data.frame() %>%
    mutate(site = rownames(.)) %>%
    pivot_longer(starts_with("X"),
                 names_to = "visit",
                 names_prefix = "X")
  y_new_mat[i, ] <- y_new_long$value
}

ppc_scatter_avg(y = y_long$value, yrep = y_new_mat, alpha = 0.3)

ppc_rootogram(y = y_long$value, yrep = y_new_mat, style = "standing")
# ppc_rootogram(y = y_long$value, yrep = y_new_mat, style = "hanging")
# ppc_rootogram(y = y_long$value, yrep = y_new_mat, stayle = "suspended")
ppc_bars(y = y_long$value, yrep = y_new_mat)
ppc_bars_grouped(y = y_long$value, yrep = y_new_mat, group = y_long$visit)
```

RMSE of posterior predictive for observations per visit

```{r}
sqrt(sum(((y_long$value - colMeans(y_new_mat))^2) / nrow(y_sum)))
```

RMSE of posterior predictive

```{r}
sqrt(sum(((y_long$value - colMeans(y_new_mat))^2) / nrow(y_long)))
```

<!--
## Prior vs. Postior Distributions 
-->

<!--
#----- Leave one (site) out cross validation -----

# print(site_od_full_pjor, par = "log_lik", digits = 2)

# Extract pointwise log-likelihood and compute LOO
log_lik_1 <- extract_log_lik(site_od_full_pjor, parameter_name = "log_lik", merge_chains = FALSE)

# remove all -inf

# loo
r_eff <- relative_eff(exp(log_lik_1)) 
loo_od <- loo::loo(log_lik_1, r_eff = r_eff, cores = nc)
print(loo_od)
pareto_k_table(loo_od)

loo_od <- loo::loo(log_lik_1, r_eff = r_eff, cores = parallel::detectCores(), K = 10)
loo_od
plot(loo_od)

# yrep <- posterior_predict(site_od_full_pjor)
# 
# ppc_loo_pit_overlay(
#   y = roaches$y,
#   yrep = yrep,
#   lw = weights(loo1$psis_object)
# )

psis_od <- psis(log_lik_1, r_eff = r_eff, cores = nc)
print(psis_od)
pareto_k_table(psis_od)
plot(psis_od, label_points = TRUE)

loo_od <- list(loo = loo_od, psis = psis_od, r_eff = r_eff)
saveRDS(loo_od, file = "Results/Stan/site_od_full_pjor_loo.Rds")


#----- Other -----

list_of_draws <- rstan::extract(site_od_full_pjor)
print(names(list_of_draws))

print(get_elapsed_time(site_od_full_pjor))

sampler_params <- get_sampler_params(site_od_full_pjor, inc_warmup = FALSE)
mean_accept_stat_by_chain <- sapply(sampler_params, function(x) mean(x[, "accept_stat__"]))
print(mean_accept_stat_by_chain)

str(sampler_params)

sapply(sampler_params, function(x) mean(x[, "energy__"]))

energy__1 = as.data.frame(sampler_params[[1]])$energy__
alpha0 <- unlist(site_od_full_pjor@sim$samples[[1]]["alpha0"])[(nb+1):ni]
alpha1 <- unlist(site_od_full_pjor@sim$samples[[1]]["alpha1"])[(nb+1):ni]
alpha2 <- unlist(site_od_full_pjor@sim$samples[[1]]["alpha2"])[(nb+1):ni]
beta0 <- unlist(site_od_full_pjor@sim$samples[[1]]["beta0"])[(nb+1):ni]
sd_eps <- unlist(site_od_full_pjor@sim$samples[[1]]["sd_eps"])[(nb+1):ni]
sd_p <- unlist(site_od_full_pjor@sim$samples[[1]]["sd_p"])[(nb+1):ni]
lp <- unlist(site_od_full_pjor@sim$samples[[1]]["lp__"])[(nb+1):ni]
pairs(data.frame(energy__1, alpha0, alpha1, alpha2, beta0, sd_eps, sd_p))
-->

\newpage

## *Desmognathus wrighti*

```{r}
#----- Load Data and Results -----
site_od_full_dwri <- readRDS(file = "Results/Stan/final_od_dwri_hmc.Rds")
```

## View Traceplots

```{r}
traceplot(site_od_full_dwri, par = c("alpha0", "alpha1", "alpha2", "alpha3", "alpha4", "alpha5", "alpha6", "beta0", "beta1", "beta2", "beta3", "beta4", "beta5", "beta6", "sd_eps", "sd_p"))
```

## Check Domain Specific Expectations

### Check N for Truncation 

The augmentation to marginalize N out as a latent discrete requires setting an upper bound, K, to loop through. If K is too small the posterior will be truncated. Need to check for every N.

```{r}
# Too many N for one plot layout so need to separate into chunks of 16 - 24
N_min <- apply(DWRI5, 1, max) # max caught on any day, so know true N is at least that large

N <- rep(NA_character_, 159)
for(i in 1:159) {
  N[i] <- paste0("N[", i, "]")
}

n <- 25

for(i in 1:(ceiling(159 / n))) {
  k <- min(i*n, 159)
  j <- k - n
  hists <- mcmc_hist(site_od_full_dwri, pars= N[j:k], binwidth = 1)
  ggsave(hists, file = paste0("Results/Stan/dwri_N_hist", i, ".pdf"))
  print(hists)
}
dev.off()
```

### Check detection

```{r}
mcmc_dens(site_od_full_dwri, pars = c("mean_detection"))
```

## Check Divergences and Pairwise Correlations

```{r}
check_divergences(site_od_full_dwri)

color_scheme_set("darkgray")

# look at variance terms because usually the hardest to fit
color_scheme_set("darkgray")

mcmc_pairs(
  site_od_full_dwri,
  pars = c("alpha0", "alpha1", "alpha2", "beta0", "sd_eps", "sd_p", "lp__"), 
  off_diag_args = list(alpha = 0.1),
  np = nuts_params(site_od_full_dwri), 
  np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
)
```

## Check Energy and Treedepth

```{r}
check_energy(site_od_full_dwri)

check_treedepth(site_od_full_dwri)
```

## Summarize Samples Sizes and Mixing

Effective samples sizes using rstan::monitor following Hoffman and Gelman (2014) to be more reliable and accurate (as in Monnahan et al. 2017) - UPDATE - now follow Vehtari et al. 2019.

effective sample sizes (should be > 100) - Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian BÃ¼rkner (2019). Rank-normalization, folding, and localization: An improved R-hat for assessing convergence of MCMC. arXiv preprint arXiv:1903.08008.

```{r, results='hide'}
mon <- rstan::monitor(site_od_full_dwri, warmup = nb)
# rstan:::print.simsummary(mon)
```

```{r}
max(mon$Rhat)
min(mon$Bulk_ESS)
min(mon$Tail_ESS)
```

## Posterior Predictive Checks

<!--
# Bayesian p-value check
plot(site_od_full_dwri, par = c("fit", "fit_new"))
pairs(site_od_full_dwri, pars = c("fit", "fit_new"))

fit <- rstan::extract(site_od_full_dwri, par = "fit")[[1]]
fit_new <- rstan::extract(site_od_full_dwri, par = "fit_new")[[1]]
# plot(fit, fit_new)

mean(fit_new > fit) # Bayesian p-value - not sure why it's much worse than JAGS output - especially in light of posterior predictive checks being great below
-->

Examine posterior prredictions of total counts across all 5 visits

```{r}
y_sum <- data.frame(transect = rownames(DWRI5), y_sum = rowSums(DWRI5), stringsAsFactors = FALSE)
y_sum_new <- rstan::extract(site_od_full_dwri, pars = c("y_new_sum"))

ppc_scatter_avg(y = y_sum$y_sum, yrep = y_sum_new[[1]]) + xlab("Mean expected count") + ylab("Observed count") + ggtitle("D. wrighti") + theme(plot.title = element_text(face = "italic")) # average posterior across samples
ggsave("Results/Stan/ppc_dwri.pdf", dpi = 1000)

# ppc_scatter(y = y_sum$y_sum, yrep = y_sum_new[[1]][sample(1:nrow(y_sum_new[[1]]), size = 16, replace = FALSE) , ]) # iteration specific for 16 random otherwise too hard to plot. Maybe could overlay but probably unnecessary to see all iterations when things generally look very good
```

RMSE of posterior predictive

```{r}
sqrt(sum(((y_sum$y_sum - colMeans(y_sum_new[[1]]))^2) / nrow(y_sum)))
```

Posterior predictive check for each visit

```{r}
y_long <- DWRI5 %>%
  mutate(site = rownames(.)) %>%
  pivot_longer(starts_with("dwri"),
               names_to = "visit",
               names_prefix = "dwri")

y_new<- rstan::extract(site_od_full_dwri, pars = c("y_new"))[[1]]

y_new_mat <- matrix(NA_integer_, dim(y_new)[1], nrow(y_long))
for(i in 1:dim(y_new)[1]) {
  y_new_long <- y_new[i, , ] %>%
    data.frame() %>%
    mutate(site = rownames(.)) %>%
    pivot_longer(starts_with("X"),
                 names_to = "visit",
                 names_prefix = "X")
  y_new_mat[i, ] <- y_new_long$value
}

ppc_scatter_avg(y = y_long$value, yrep = y_new_mat, alpha = 0.3)
```

RMSE of posterior predictive for observations per visit

```{r}
sqrt(sum(((y_long$value - colMeans(y_new_mat))^2) / nrow(y_sum)))
```

RMSE of posterior predictive

```{r}
sqrt(sum(((y_long$value - colMeans(y_new_mat))^2) / nrow(y_long)))
```

\newpage

## *Eurycea wilderae*

```{r}
#----- Load Data and Results -----
site_od_full_ewil <- readRDS(file = "Results/Stan/final_od_ewil_hmc.Rds")
```

## View Traceplots

```{r}
traceplot(site_od_full_ewil, par = c("alpha0", "alpha1", "alpha2", "alpha3", "alpha4", "alpha5", "alpha6", "beta0", "beta1", "beta2", "beta3", "beta4", "beta5", "beta6", "sd_eps", "sd_p"))
```

## Check Domain Specific Expectations

### Check N for Truncation 

The augmentation to marginalize N out as a latent discrete requires setting an upper bound, K, to loop through. If K is too small the posterior will be truncated. Need to check for every N.

```{r}
# Too many N for one plot layout so need to separate into chunks of 16 - 24
N_min <- apply(EWIL5, 1, max) # max caught on any day, so know true N is at least that large

N <- rep(NA_character_, 159)
for(i in 1:159) {
  N[i] <- paste0("N[", i, "]")
}

n <- 25

for(i in 1:(ceiling(159 / n))) {
  k <- min(i*n, 159)
  j <- k - n
  hists <- mcmc_hist(site_od_full_ewil, pars= N[j:k], binwidth = 1)
  ggsave(hists, file = paste0("Results/Stan/ewil_N_hist", i, ".pdf"))
  print(hists)
}
dev.off()
```

### Check detection

```{r}
mcmc_dens(site_od_full_ewil, pars = c("mean_detection"))
```

## Check Divergences and Pairwise Correlations

```{r}
check_divergences(site_od_full_ewil)

color_scheme_set("darkgray")

# look at variance terms because usually the hardest to fit
color_scheme_set("darkgray")

mcmc_pairs(
  site_od_full_ewil,
  pars = c("alpha0", "alpha1", "alpha2", "beta0", "sd_eps", "sd_p", "lp__"), 
  off_diag_args = list(alpha = 0.1),
  np = nuts_params(site_od_full_ewil), 
  np_style = scatter_style_np(div_color = "green", div_alpha = 0.8)
)
```

## Check Energy and Treedepth

```{r}
check_energy(site_od_full_ewil)

check_treedepth(site_od_full_ewil)
```

## Summarize Samples Sizes and Mixing

Effective samples sizes using rstan::monitor following Hoffman and Gelman (2014) to be more reliable and accurate (as in Monnahan et al. 2017) - UPDATE - now follow Vehtari et al. 2019.

effective sample sizes (should be > 100) - Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian BÃ¼rkner (2019). Rank-normalization, folding, and localization: An improved R-hat for assessing convergence of MCMC. arXiv preprint arXiv:1903.08008.

```{r, results='hide'}
mon <- rstan::monitor(site_od_full_ewil, warmup = nb)
# rstan:::print.simsummary(mon)
```

```{r}
max(mon$Rhat)
min(mon$Bulk_ESS)
min(mon$Tail_ESS)
```

## Posterior Predictive Checks

<!--
# Bayesian p-value check
plot(site_od_full_ewil, par = c("fit", "fit_new"))
pairs(site_od_full_ewil, pars = c("fit", "fit_new"))

fit <- rstan::extract(site_od_full_ewil, par = "fit")[[1]]
fit_new <- rstan::extract(site_od_full_ewil, par = "fit_new")[[1]]
# plot(fit, fit_new)

mean(fit_new > fit) # Bayesian p-value - not sure why it's much worse than JAGS output - especially in light of posterior predictive checks being great below
-->

Examine posterior prredictions of total counts across all 5 visits

```{r}
y_sum <- data.frame(transect = rownames(EWIL5), y_sum = rowSums(EWIL5), stringsAsFactors = FALSE)
y_sum_new <- rstan::extract(site_od_full_ewil, pars = c("y_new_sum"))

ppc_scatter_avg(y = y_sum$y_sum, yrep = y_sum_new[[1]]) + xlab("Mean expected count") + ylab("Observed count") + ggtitle("E. wilderae") + theme(plot.title = element_text(face = "italic")) # average posterior across samples
ggsave("Results/Stan/ppc_ewil.pdf", dpi = 1000)

# ppc_scatter(y = y_sum$y_sum, yrep = y_sum_new[[1]][sample(1:nrow(y_sum_new[[1]]), size = 16, replace = FALSE) , ]) # iteration specific for 16 random otherwise too hard to plot. Maybe could overlay but probably unnecessary to see all iterations when things generally look very good
```

RMSE of posterior predictive

```{r}
sqrt(sum(((y_sum$y_sum - colMeans(y_sum_new[[1]]))^2) / nrow(y_sum)))
```

Posterior predictive check for each visit

```{r}
y_long <- EWIL5 %>%
  mutate(site = rownames(.)) %>%
  pivot_longer(starts_with("ewil"),
               names_to = "visit",
               names_prefix = "ewil")

y_new<- rstan::extract(site_od_full_ewil, pars = c("y_new"))[[1]]

y_new_mat <- matrix(NA_integer_, dim(y_new)[1], nrow(y_long))
for(i in 1:dim(y_new)[1]) {
  y_new_long <- y_new[i, , ] %>%
    data.frame() %>%
    mutate(site = rownames(.)) %>%
    pivot_longer(starts_with("X"),
                 names_to = "visit",
                 names_prefix = "X")
  y_new_mat[i, ] <- y_new_long$value
}

ppc_scatter_avg(y = y_long$value, yrep = y_new_mat, alpha = 0.3)
```

RMSE of posterior predictive for observations per visit

```{r}
sqrt(sum(((y_long$value - colMeans(y_new_mat))^2) / nrow(y_sum)))
```

RMSE of posterior predictive

```{r}
sqrt(sum(((y_long$value - colMeans(y_new_mat))^2) / nrow(y_long)))
```
